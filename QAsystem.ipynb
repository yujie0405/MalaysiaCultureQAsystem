{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cf4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Gemma3 model and tokenizer...\n",
      "Error loading Gemma3 model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2-9b-it.\n",
      "401 Client Error. (Request ID: Root=1-685bf297-39b141082b15cbf22e7b43df;8c7c9439-0e6b-4e48-9ceb-e53c6596b7fa)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2-9b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2-9b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Fallback model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, messagebox\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import json\n",
    "from googletrans import Translator\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "class Gemma3QAInference:\n",
    "    def __init__(self, model_path: str = \"gemma3\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load Gemma3 model and tokenizer\n",
    "        print(\"Loading Gemma3 model and tokenizer...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(\"Gemma3 model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Gemma3 model: {e}\")\n",
    "            # Fallback to a smaller model if available\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "                self.model.to(self.device)\n",
    "                self.model.eval()\n",
    "                print(\"Fallback model loaded successfully!\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error loading fallback model: {e2}\")\n",
    "                raise e2\n",
    "        \n",
    "        # Initialize translator\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # Load QA dataset (Malay only)\n",
    "        self.qa_data = self.load_qa_data(\"qa_train.json\")\n",
    "    \n",
    "    def translate(self, text, src='auto', dest='ms'):\n",
    "        \"\"\"Translate text using Google Translate.\"\"\"\n",
    "        try:\n",
    "            result = self.translator.translate(text, src=src, dest=dest)\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "            return text\n",
    "    \n",
    "    def find_similar_question(self, question, threshold=0.3):\n",
    "        \"\"\"\n",
    "        Find the most similar question in the training data.\n",
    "        \n",
    "        Args:\n",
    "            question: The input question\n",
    "            threshold: Minimum similarity threshold\n",
    "            \n",
    "        Returns:\n",
    "            Best matching training example or None\n",
    "        \"\"\"\n",
    "        if not self.qa_data:\n",
    "            return None\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Clean the input question\n",
    "        question_clean = re.sub(r'[^\\w\\s]', '', question.lower())\n",
    "        \n",
    "        for example in self.qa_data:\n",
    "            # Clean the training question\n",
    "            train_question_clean = re.sub(r'[^\\w\\s]', '', example['question'].lower())\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = SequenceMatcher(None, question_clean, train_question_clean).ratio()\n",
    "            \n",
    "            # Also check for keyword overlap\n",
    "            question_words = set(question_clean.split())\n",
    "            train_words = set(train_question_clean.split())\n",
    "            keyword_overlap = len(question_words.intersection(train_words)) / max(len(question_words), 1)\n",
    "            \n",
    "            # Combined score\n",
    "            combined_score = (similarity + keyword_overlap) / 2\n",
    "            \n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_match = example\n",
    "        \n",
    "        return best_match if best_score >= threshold else None\n",
    "    \n",
    "    def load_qa_data(self, file_path):\n",
    "        \"\"\"Load Malay QA data from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # Ensure we have a list of QA pairs\n",
    "                if isinstance(data, dict):\n",
    "                    return [data]  # If single QA pair, make it a list\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading QA data: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_context_for_question(self, question: str):\n",
    "        # Try to find similar question in training data\n",
    "        similar_example = self.find_similar_question(question)\n",
    "        \n",
    "        if similar_example:\n",
    "            return similar_example\n",
    "        \n",
    "        # Fallback: search for keywords in training data\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Extract potential keywords from the question\n",
    "        keywords = []\n",
    "        for word in question_lower.split():\n",
    "            if len(word) > 3:  # Only consider words longer than 3 characters\n",
    "                keywords.append(word)\n",
    "        \n",
    "        # Search for examples containing these keywords\n",
    "        for example in self.qa_data:\n",
    "            context_lower = example['context'].lower()\n",
    "            question_lower = example['question'].lower()\n",
    "            \n",
    "            # Check if any keyword appears in context or question\n",
    "            for keyword in keywords:\n",
    "                if keyword in context_lower or keyword in question_lower:\n",
    "                    return example\n",
    "        \n",
    "        # Final fallback: return a generic context\n",
    "        return {\n",
    "            'context': \"Budaya Cina Malaysia adalah sebahagian penting dalam masyarakat Malaysia yang telah diwarisi dari generasi ke generasi.\",\n",
    "            'question': question,\n",
    "            'answer': \"Budaya Cina Malaysia merangkumi pelbagai aspek termasuk perayaan, makanan, dan adat istiadat yang telah diwarisi dari generasi ke generasi.\"\n",
    "        }\n",
    "    \n",
    "    def ask_question(self, question: str, context: str, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        Return the context as the answer (no LLM generation)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Return the context directly as the answer\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"answer\": context,  # Use context as the answer\n",
    "                \"confidence\": 1.0,  # 100% confidence since we're using the exact context\n",
    "                \"start_confidence\": 1.0,\n",
    "                \"end_confidence\": 1.0,\n",
    "                \"answer_start\": 0,\n",
    "                \"answer_end\": len(context)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"answer\": \"Maaf, berlaku ralat semasa memproses soalan anda.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"start_confidence\": 0.0,\n",
    "                \"end_confidence\": 0.0,\n",
    "                \"answer_start\": 0,\n",
    "                \"answer_end\": 0\n",
    "            }\n",
    "\n",
    "class QAGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Gemma3 Question Answering System\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        \n",
    "        # Initialize QA system\n",
    "        self.qa_system = None\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        self.setup_ui()\n",
    "        self.load_model()\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Setup the user interface.\"\"\"\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Configure grid weights\n",
    "        self.root.columnconfigure(0, weight=1)\n",
    "        self.root.rowconfigure(0, weight=1)\n",
    "        main_frame.columnconfigure(0, weight=1)\n",
    "        main_frame.rowconfigure(2, weight=1)\n",
    "\n",
    "        # Title\n",
    "        title_label = ttk.Label(main_frame, text=\"ü§ñ Gemma3 Question Answering System\", \n",
    "                               font=(\"Arial\", 16, \"bold\"))\n",
    "        title_label.grid(row=0, column=0, pady=(0, 20))\n",
    "        \n",
    "        # Model status\n",
    "        self.status_label = ttk.Label(main_frame, text=\"Loading model...\", \n",
    "                                     foreground=\"orange\")\n",
    "        self.status_label.grid(row=1, column=0, pady=(0, 20))\n",
    "        \n",
    "        # Question input\n",
    "        question_frame = ttk.LabelFrame(main_frame, text=\"Ask Your Question\", padding=\"10\")\n",
    "        question_frame.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)\n",
    "        question_frame.columnconfigure(0, weight=1)\n",
    "        \n",
    "        ttk.Label(question_frame, text=\"Question:\").grid(row=0, column=0, sticky=tk.W, pady=5)\n",
    "        self.question_entry = ttk.Entry(question_frame, width=80, font=(\"Arial\", 12))\n",
    "        self.question_entry.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=5)\n",
    "        \n",
    "        # Ask button\n",
    "        self.ask_button = ttk.Button(question_frame, text=\"Ask Question\", \n",
    "                                   command=self.ask_question, style=\"Accent.TButton\")\n",
    "        self.ask_button.grid(row=2, column=0, pady=20)\n",
    "        \n",
    "        # Results area\n",
    "        results_frame = ttk.LabelFrame(main_frame, text=\"Answer\", padding=\"10\")\n",
    "        results_frame.grid(row=3, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)\n",
    "        results_frame.columnconfigure(0, weight=1)\n",
    "        results_frame.rowconfigure(0, weight=1)\n",
    "        \n",
    "        self.results_text = scrolledtext.ScrolledText(results_frame, height=15, width=80, \n",
    "                                                    font=(\"Arial\", 11))\n",
    "        self.results_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Example questions button\n",
    "        ttk.Button(main_frame, text=\"Load Example Questions\", \n",
    "                  command=self.load_examples).grid(row=4, column=0, pady=10)\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the Gemma3 model.\"\"\"\n",
    "        try:\n",
    "            self.qa_system = Gemma3QAInference()\n",
    "            self.model_loaded = True\n",
    "            \n",
    "            # Show model and training data status\n",
    "            training_count = len(self.qa_system.qa_data) if self.qa_system.qa_data else 0\n",
    "            status_text = f\"‚úÖ Gemma3 model loaded successfully! | üìö {training_count} training examples loaded\"\n",
    "            self.status_label.config(text=status_text, foreground=\"green\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_label.config(text=f\"‚ùå Error loading model: {str(e)}\", foreground=\"red\")\n",
    "            self.ask_button.config(state=\"disabled\")\n",
    "    \n",
    "    def ask_question(self):\n",
    "        \"\"\"Ask a question and display the result.\"\"\"\n",
    "        if not self.model_loaded:\n",
    "            messagebox.showerror(\"Error\", \"Model not loaded!\")\n",
    "            return\n",
    "        \n",
    "        question = self.question_entry.get().strip()\n",
    "        \n",
    "        if not question:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a question!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Show loading indicator and disable button\n",
    "            self.results_text.delete(\"1.0\", tk.END)\n",
    "            self.results_text.insert(\"1.0\", \"Loading...\\n\")\n",
    "            self.ask_button.config(state=\"disabled\")\n",
    "            self.root.update_idletasks()\n",
    "            \n",
    "            # Use googletrans to detect language\n",
    "            detected = self.qa_system.translator.detect(question)\n",
    "            lang = detected.lang\n",
    "            if lang == 'en':\n",
    "                # English: translate to Malay for inference, then back to English for display\n",
    "                question_ms = self.qa_system.translate(question, src='en', dest='ms')\n",
    "                context_data = self.qa_system.get_context_for_question(question_ms)\n",
    "                result_ms = self.qa_system.ask_question(question_ms, context_data['context'])\n",
    "                context_en = self.qa_system.translate(result_ms['context'], src='ms', dest='en')\n",
    "                answer_en = self.qa_system.translate(result_ms['answer'], src='ms', dest='en')\n",
    "                result = result_ms.copy()\n",
    "                result['context'] = context_en\n",
    "                result['answer'] = answer_en\n",
    "                result['source'] = 'model'\n",
    "            else:\n",
    "                # Malay or other: use as is\n",
    "                context_data = self.qa_system.get_context_for_question(question)\n",
    "                result = self.qa_system.ask_question(question, context_data['context'])\n",
    "                result['source'] = 'model'\n",
    "                \n",
    "                # Try to find a better answer from training data\n",
    "                similar_example = self.qa_system.find_similar_question(question)\n",
    "                if similar_example and result['confidence'] < 0.5:\n",
    "                    # If model confidence is low, use training data answer\n",
    "                    result['answer'] = similar_example['answer']\n",
    "                    result['context'] = similar_example['context']\n",
    "                    result['confidence'] = 0.8  # Higher confidence for training data\n",
    "                    result['start_confidence'] = 0.8\n",
    "                    result['end_confidence'] = 0.8\n",
    "                    result['answer_start'] = 0\n",
    "                    result['answer_end'] = len(similar_example['answer'])\n",
    "                    result['source'] = 'training_data'\n",
    "            \n",
    "            # Ensure we show the original question in results\n",
    "            result['question'] = question\n",
    "            \n",
    "            # Display result\n",
    "            self.display_result(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing question: {str(e)}\"\n",
    "            messagebox.showerror(\"Error\", error_msg)\n",
    "            self.ask_button.config(state=\"normal\")\n",
    "            return\n",
    "        \n",
    "        # Re-enable button after displaying result\n",
    "        self.ask_button.config(state=\"normal\")\n",
    "    \n",
    "    def display_result(self, result):\n",
    "        \"\"\"Display the result in the results area.\"\"\"\n",
    "        self.results_text.delete(\"1.0\", tk.END)\n",
    "        \n",
    "        # Determine source indicator\n",
    "        source_indicator = \"ü§ñ Model Prediction\" if result.get('source') == 'model' else \"üìö Training Data\"\n",
    "        \n",
    "        output = f\"\"\"üìù Question: {result['question']}\n",
    "\n",
    "‚úÖ Answer: {result['answer']}\n",
    "\n",
    "üéØ Confidence: {result['confidence']:.2%}\n",
    "{source_indicator}\n",
    "\n",
    "üìä Details:\n",
    "- Start Confidence: {result['start_confidence']:.2%}\n",
    "- End Confidence: {result['end_confidence']:.2%}\n",
    "- Answer Span: {result['answer_start']} to {result['answer_end']}\n",
    "\"\"\"\n",
    "        \n",
    "        self.results_text.insert(\"1.0\", output)\n",
    "    \n",
    "    def load_examples(self):\n",
    "        \"\"\"Load example questions from training data.\"\"\"\n",
    "        # Get unique questions from training data\n",
    "        unique_questions = []\n",
    "        seen_questions = set()\n",
    "        \n",
    "        if self.qa_system and self.qa_system.qa_data:\n",
    "            for example in self.qa_system.qa_data:\n",
    "                question = example['question']\n",
    "                if question not in seen_questions:\n",
    "                    unique_questions.append(question)\n",
    "                    seen_questions.add(question)\n",
    "                    if len(unique_questions) >= 10:  # Limit to 10 examples\n",
    "                        break\n",
    "        \n",
    "        # Fallback examples if no training data\n",
    "        if not unique_questions:\n",
    "            unique_questions = [\n",
    "                \"Apakah Tahun Baru Cina?\",\n",
    "                \"Apakah Angpau?\",\n",
    "                \"Apakah Kuih Bulan?\",\n",
    "                \"Bagaimana Pesta Tanglung disambut?\",\n",
    "                \"Apakah kepentingan Masakan Cina Malaysia?\",\n",
    "                \"Bagaimana Wayang Kulit dipersembahkan?\",\n",
    "                \"Apakah maksud Kongsi Raya?\",\n",
    "                \"Bagaimana budaya Cina diwarisi?\",\n",
    "                \"Apakah ciri-ciri perayaan Cina?\",\n",
    "                \"Bagaimana makanan tradisional Cina disediakan?\"\n",
    "            ]\n",
    "        \n",
    "        # Create example window\n",
    "        example_window = tk.Toplevel(self.root)\n",
    "        example_window.title(\"Example Questions\")\n",
    "        example_window.geometry(\"600x400\")\n",
    "        \n",
    "        # Example list\n",
    "        example_frame = ttk.Frame(example_window, padding=\"10\")\n",
    "        example_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        ttk.Label(example_frame, text=\"Click an example to load it:\", \n",
    "                 font=(\"Arial\", 12, \"bold\")).pack(pady=(0, 10))\n",
    "        \n",
    "        for i, example in enumerate(unique_questions, 1):\n",
    "            btn = ttk.Button(example_frame, text=f\"{i}. {example}\", \n",
    "                           command=lambda ex=example: self.load_example(ex))\n",
    "            btn.pack(fill=tk.X, pady=2)\n",
    "    \n",
    "    def load_example(self, example):\n",
    "        \"\"\"Load an example into the question field.\"\"\"\n",
    "        self.question_entry.delete(0, tk.END)\n",
    "        self.question_entry.insert(0, example)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the GUI.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    app = QAGUI(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b6506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
