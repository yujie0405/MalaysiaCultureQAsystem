{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brian chin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rdflib\\plugins\\serializers\\nt.py:41: UserWarning: NTSerializer always uses UTF-8 encoding. Given encoding was: None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge graph creation complete:\n",
      "- Entities: 4536\n",
      "- Relationships: 24505\n",
      "- Source documents: 1048\n",
      "\n",
      "Generated files:\n",
      "- TURTLE: Knowledge representation/knowledge_graphs\\knowledge_representation_kg_20250624_121810.ttl\n",
      "- JSONLD: Knowledge representation/knowledge_graphs\\knowledge_representation_kg_20250624_121810.jsonld\n",
      "- NTRIPLES: Knowledge representation/knowledge_graphs\\knowledge_representation_kg_20250624_121810.nt\n",
      "- SIMPLIFIED: Knowledge representation/knowledge_graphs\\knowledge_representation_kg_simplified_20250624_121810.json\n",
      "\n",
      "Knowledge graph created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from rdflib import Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, XSD, FOAF, DCTERMS\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def create_knowledge_graph(json_file_path: str, output_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates a knowledge graph from a JSON knowledge representation file.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON knowledge base file\n",
    "        output_dir: Directory to save the output knowledge graph files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with paths to the generated files\n",
    "    \"\"\"\n",
    "    # Load the JSON knowledge base\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            knowledge_base = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load JSON file: {str(e)}\")\n",
    "    \n",
    "    # Create RDF Graph\n",
    "    g = Graph()\n",
    "    \n",
    "    # Define namespaces - FIXED: Properly define all namespaces\n",
    "    ns = Namespace(\"http://example.org/chinese-culture/\")\n",
    "    dbo = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "    schema = Namespace(\"http://schema.org/\")\n",
    "    prov = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "    \n",
    "    g.bind(\"ns\", ns)\n",
    "    g.bind(\"dbo\", dbo)\n",
    "    g.bind(\"schema\", schema)\n",
    "    g.bind(\"prov\", prov)\n",
    "    \n",
    "    # Create a dictionary to map predicates to more semantic relationships\n",
    "    PREDICATE_MAPPING = {\n",
    "        \"related_to\": ns.relatedTo,\n",
    "        # Add more specific mappings as needed\n",
    "        # \"born_in\": schema.birthPlace,\n",
    "        # \"died_in\": schema.deathPlace,\n",
    "    }\n",
    "    \n",
    "    # Track entities and their types for better classification\n",
    "    entity_types = {}\n",
    "    \n",
    "    # Process each document chunk\n",
    "    for chunk in knowledge_base:\n",
    "        # Create document entity with provenance information\n",
    "        doc_uri = ns[f\"document/{quote(chunk['id'])}\"]\n",
    "        g.add((doc_uri, RDF.type, schema.Article))\n",
    "        g.add((doc_uri, DCTERMS.source, Literal(chunk['source'])))\n",
    "        g.add((doc_uri, schema.text, Literal(chunk['text'], lang=\"ms\")))\n",
    "        g.add((doc_uri, DCTERMS.created, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))\n",
    "        \n",
    "        # Process triples\n",
    "        for triple in chunk['triples']:\n",
    "            if len(triple) != 3:\n",
    "                continue  # Skip malformed triples\n",
    "                \n",
    "            subject = triple[0].strip()\n",
    "            predicate = triple[1].strip()\n",
    "            obj = triple[2].strip()\n",
    "            \n",
    "            # Skip empty components\n",
    "            if not subject or not predicate or not obj:\n",
    "                continue\n",
    "                \n",
    "            # Create URIs for subject and object\n",
    "            subject_uri = ns[f\"entity/{quote(subject)}\"]\n",
    "            obj_uri = ns[f\"entity/{quote(obj)}\"]\n",
    "            \n",
    "            # Add subject and object as entities with labels\n",
    "            g.add((subject_uri, RDF.type, dbo.Thing))\n",
    "            g.add((subject_uri, RDFS.label, Literal(subject, lang=\"ms\")))\n",
    "            \n",
    "            g.add((obj_uri, RDF.type, dbo.Thing))\n",
    "            g.add((obj_uri, RDFS.label, Literal(obj, lang=\"ms\")))\n",
    "            \n",
    "            # Try to infer types based on content\n",
    "            infer_entity_type(subject_uri, subject, g, ns, dbo)\n",
    "            infer_entity_type(obj_uri, obj, g, ns, dbo)\n",
    "            \n",
    "            # Add relationship between them\n",
    "            rel_uri = PREDICATE_MAPPING.get(predicate, ns[predicate])\n",
    "            g.add((subject_uri, rel_uri, obj_uri))\n",
    "            \n",
    "            # Add inverse relationship when appropriate\n",
    "            if predicate == \"related_to\":\n",
    "                g.add((obj_uri, rel_uri, subject_uri))\n",
    "            \n",
    "            # Link document to the entities\n",
    "            g.add((doc_uri, schema.about, subject_uri))\n",
    "            g.add((doc_uri, schema.about, obj_uri))\n",
    "            g.add((subject_uri, prov.wasDerivedFrom, doc_uri))\n",
    "            g.add((obj_uri, prov.wasDerivedFrom, doc_uri))\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate output filenames\n",
    "    base_filename = os.path.splitext(os.path.basename(json_file_path))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save in multiple formats\n",
    "    output_files = {}\n",
    "    \n",
    "    # Turtle format\n",
    "    ttl_path = os.path.join(output_dir, f\"{base_filename}_kg_{timestamp}.ttl\")\n",
    "    g.serialize(destination=ttl_path, format='turtle')\n",
    "    output_files['turtle'] = ttl_path\n",
    "    \n",
    "    # JSON-LD format\n",
    "    jsonld_path = os.path.join(output_dir, f\"{base_filename}_kg_{timestamp}.jsonld\")\n",
    "    g.serialize(destination=jsonld_path, format='json-ld')\n",
    "    output_files['jsonld'] = jsonld_path\n",
    "    \n",
    "    # N-Triples format\n",
    "    nt_path = os.path.join(output_dir, f\"{base_filename}_kg_{timestamp}.nt\")\n",
    "    g.serialize(destination=nt_path, format='nt')\n",
    "    output_files['ntriples'] = nt_path\n",
    "    \n",
    "    # Create a simplified JSON representation\n",
    "    simplified_kg = create_simplified_kg(g, knowledge_base, schema, dbo, prov)\n",
    "    simplified_path = os.path.join(output_dir, f\"{base_filename}_kg_simplified_{timestamp}.json\")\n",
    "    with open(simplified_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(simplified_kg, f, indent=2, ensure_ascii=False)\n",
    "    output_files['simplified'] = simplified_path\n",
    "    \n",
    "    print(f\"\\nKnowledge graph creation complete:\")\n",
    "    print(f\"- Entities: {len(simplified_kg['entities'])}\")\n",
    "    print(f\"- Relationships: {len(simplified_kg['relationships'])}\")\n",
    "    print(f\"- Source documents: {len(simplified_kg['documents'])}\")\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for fmt, path in output_files.items():\n",
    "        print(f\"- {fmt.upper()}: {path}\")\n",
    "    \n",
    "    return output_files\n",
    "\n",
    "def infer_entity_type(entity_uri, entity_name: str, graph: Graph, ns: Namespace, dbo: Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Attempts to infer a more specific type for an entity based on its name.\n",
    "    \"\"\"\n",
    "    # Simple type inference based on patterns in the name\n",
    "    lower_name = entity_name.lower()\n",
    "    \n",
    "    if any(x in lower_name for x in ['festival', 'perayaan']):\n",
    "        graph.add((entity_uri, RDF.type, ns.Festival))\n",
    "    elif any(x in lower_name for x in ['dynasty', 'dinasti']):\n",
    "        graph.add((entity_uri, RDF.type, dbo.HistoricalPeriod))\n",
    "    elif any(x in lower_name for x in ['city', 'bandar', 'kota']):\n",
    "        graph.add((entity_uri, RDF.type, dbo.City))\n",
    "    elif any(x in lower_name for x in ['country', 'negara']):\n",
    "        graph.add((entity_uri, RDF.type, dbo.Country))\n",
    "    elif any(x in lower_name for x in ['person', 'orang', 'tokoh']):\n",
    "        graph.add((entity_uri, RDF.type, dbo.Person))\n",
    "    elif any(x in lower_name for x in ['culture', 'budaya']):\n",
    "        graph.add((entity_uri, RDF.type, dbo.Culture))\n",
    "    # Add more type inference rules as needed\n",
    "\n",
    "def create_simplified_kg(graph: Graph, original_kb: List[Dict], schema: Namespace, dbo: Namespace, prov: Namespace) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a simplified JSON representation of the knowledge graph.\n",
    "    \"\"\"\n",
    "    simplified = {\n",
    "        \"metadata\": {\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"source\": \"Generated from knowledge_representation.json\",\n",
    "            \"languages\": [\"ms\"],  # Malay language\n",
    "            \"statistics\": {}\n",
    "        },\n",
    "        \"documents\": [],\n",
    "        \"entities\": [],\n",
    "        \"relationships\": []\n",
    "    }\n",
    "    \n",
    "    # Process documents\n",
    "    doc_uris = set(graph.subjects(RDF.type, schema.Article))\n",
    "    simplified['metadata']['statistics']['documents'] = len(doc_uris)\n",
    "    \n",
    "    for doc_uri in doc_uris:\n",
    "        doc_info = {\n",
    "            \"uri\": str(doc_uri),\n",
    "            \"source\": str(graph.value(doc_uri, DCTERMS.source)),\n",
    "            \"text\": str(graph.value(doc_uri, schema.text)),\n",
    "            \"entities\": []\n",
    "        }\n",
    "        simplified['documents'].append(doc_info)\n",
    "    \n",
    "    # Process entities\n",
    "    entity_uris = set(graph.subjects(RDF.type, None))\n",
    "    simplified['metadata']['statistics']['entities'] = len(entity_uris)\n",
    "    \n",
    "    for entity_uri in entity_uris:\n",
    "        # Skip documents\n",
    "        if (entity_uri, RDF.type, schema.Article) in graph:\n",
    "            continue\n",
    "            \n",
    "        labels = list(graph.objects(entity_uri, RDFS.label))\n",
    "        types = [str(t) for t in graph.objects(entity_uri, RDF.type) if t != dbo.Thing]\n",
    "        \n",
    "        entity_info = {\n",
    "            \"uri\": str(entity_uri),\n",
    "            \"label\": str(labels[0]) if labels else \"\",\n",
    "            \"types\": types,\n",
    "            \"source_documents\": list(set(\n",
    "                str(doc) for doc in graph.objects(entity_uri, prov.wasDerivedFrom)\n",
    "            ))\n",
    "        }\n",
    "        simplified['entities'].append(entity_info)\n",
    "        \n",
    "        # Link entities to documents\n",
    "        for doc in entity_info['source_documents']:\n",
    "            for doc_info in simplified['documents']:\n",
    "                if doc_info['uri'] == doc:\n",
    "                    doc_info['entities'].append(str(entity_uri))\n",
    "    \n",
    "    # Process relationships\n",
    "    rel_count = 0\n",
    "    for s, p, o in graph:\n",
    "        # Only include relationships between entities (not documents)\n",
    "        if (s, RDF.type, schema.Article) in graph or (o, RDF.type, schema.Article) in graph:\n",
    "            continue\n",
    "            \n",
    "        s_label = str(next(graph.objects(s, RDFS.label), s))\n",
    "        o_label = str(next(graph.objects(o, RDFS.label), o))\n",
    "        \n",
    "        rel_info = {\n",
    "            \"subject\": str(s),\n",
    "            \"subject_label\": s_label,\n",
    "            \"predicate\": str(p),\n",
    "            \"object\": str(o),\n",
    "            \"object_label\": o_label\n",
    "        }\n",
    "        simplified['relationships'].append(rel_info)\n",
    "        rel_count += 1\n",
    "    \n",
    "    simplified['metadata']['statistics']['relationships'] = rel_count\n",
    "    \n",
    "    return simplified\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_json = \"Knowledge representation/knowledge_representation.json\"\n",
    "    output_dir = \"Knowledge representation/knowledge_graphs\"\n",
    "    \n",
    "    try:\n",
    "        result_files = create_knowledge_graph(input_json, output_dir)\n",
    "        print(\"\\nKnowledge graph created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creating knowledge graph: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 27492 QA pairs\n",
      "Saved to: Knowledge representation/qa_datasets\\qa_train.json\n",
      "\n",
      "QA dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from random import choice, sample\n",
    "\n",
    "def generate_qa_pairs(json_file_path: str, output_dir: str, num_pairs_per_entity: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Generates question-answer pairs from a JSON knowledge representation file.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON knowledge base file\n",
    "        output_dir: Directory to save the output QA JSON file\n",
    "        num_pairs_per_entity: Number of QA pairs to generate per entity\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated QA JSON file\n",
    "    \"\"\"\n",
    "    # Load the JSON knowledge base\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            knowledge_base = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load JSON file: {str(e)}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate QA pairs\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Question templates in Malay (Bahasa Malaysia)\n",
    "    question_templates = [\n",
    "        \"Apakah {entity}?\",\n",
    "        \"Boleh anda terangkan tentang {entity}?\",\n",
    "        \"Apa yang anda tahu mengenai {entity}?\",\n",
    "        \"Apakah kepentingan {entity} dalam budaya Cina Malaysia?\",\n",
    "        \"Bagaimana {entity} berkaitan dengan budaya Cina Malaysia?\",\n",
    "        \"Apakah makna {entity} dalam masyarakat Cina Malaysia?\",\n",
    "        \"Boleh anda berikan maklumat tentang {entity}?\",\n",
    "        \"Apakah peranan {entity} dalam tradisi Cina Malaysia?\"\n",
    "    ]\n",
    "    \n",
    "    # Process each document chunk to create QA pairs\n",
    "    for chunk in knowledge_base:\n",
    "        # Extract entities and relationships from triples\n",
    "        entities = set()\n",
    "        relationships = []\n",
    "        \n",
    "        for triple in chunk['triples']:\n",
    "            if len(triple) != 3:\n",
    "                continue\n",
    "                \n",
    "            subject = triple[0].strip()\n",
    "            predicate = triple[1].strip()\n",
    "            obj = triple[2].strip()\n",
    "            \n",
    "            if subject and obj:\n",
    "                entities.add(subject)\n",
    "                entities.add(obj)\n",
    "                relationships.append((subject, predicate, obj))\n",
    "        \n",
    "        # Generate QA pairs for each entity\n",
    "        for entity in entities:\n",
    "            # Find relevant context from the original text\n",
    "            context = find_entity_context(entity, chunk['text'])\n",
    "            \n",
    "            # Find answers that mention this entity\n",
    "            possible_answers = []\n",
    "            for subj, pred, obj in relationships:\n",
    "                if entity in (subj, obj):\n",
    "                    answer_text = f\"{subj} {pred} {obj}\"\n",
    "                    possible_answers.append(answer_text)\n",
    "            \n",
    "            # If no direct relationships, use the context as answer\n",
    "            if not possible_answers and context:\n",
    "                possible_answers.append(context)\n",
    "            \n",
    "            # Generate multiple QA pairs per entity\n",
    "            for _ in range(min(num_pairs_per_entity, len(question_templates))):\n",
    "                if not possible_answers:\n",
    "                    break\n",
    "                    \n",
    "                # Select a random question template and answer\n",
    "                question_template = choice(question_templates)\n",
    "                answer = choice(possible_answers)\n",
    "                \n",
    "                # Create the QA pair\n",
    "                qa_pair = {\n",
    "                    \"question\": question_template.format(entity=entity),\n",
    "                    \"context\": context if context else \"\",\n",
    "                    \"answer\": answer,\n",
    "                    \"language\": \"ms\",\n",
    "                    \"topic\": entity\n",
    "                }\n",
    "                \n",
    "                qa_pairs.append(qa_pair)\n",
    "    \n",
    "    # Generate output filename\n",
    "    base_filename = os.path.splitext(os.path.basename(json_file_path))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = os.path.join(output_dir, f\"qa_train.json\")\n",
    "    \n",
    "    # Save the QA pairs\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nGenerated {len(qa_pairs)} QA pairs\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def find_entity_context(entity: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds relevant context for an entity in the text.\n",
    "    \"\"\"\n",
    "    sentences = text.split('.')\n",
    "    for sentence in sentences:\n",
    "        if entity in sentence:\n",
    "            return sentence.strip() + '.'\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_json = \"Knowledge representation/knowledge_representation.json\"\n",
    "    output_dir = \"Knowledge representation/qa_datasets\"\n",
    "    \n",
    "    try:\n",
    "        output_file = generate_qa_pairs(input_json, output_dir)\n",
    "        print(\"\\nQA dataset created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creating QA dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training mBERT\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 610 QA pairs\n",
      "Splitting data into train/validation sets...\n",
      "Training samples: 549\n",
      "Validation samples: 61\n",
      "Loading mBERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Brian chin\\AppData\\Local\\Temp\\ipykernel_22852\\3674536702.py:217: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Starting training for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/35 [00:00<?, ?it/s]C:\\Users\\Brian chin\\AppData\\Local\\Temp\\ipykernel_22852\\3674536702.py:236: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/5: 100%|██████████| 35/35 [08:42<00:00, 14.93s/it, loss=0.6684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 1.4484\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss:   0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\Brian chin\\AppData\\Local\\Temp\\ipykernel_22852\\3674536702.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation Loss: 100%|██████████| 4/4 [00:13<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating EM and F1 scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:11<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results:\n",
      "  Validation Loss: 0.7908\n",
      "  Exact Match: 0.0000\n",
      "  F1 Score: 0.1677\n",
      "New best F1 score! Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 35/35 [08:44<00:00, 14.99s/it, loss=0.6003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss: 0.7534\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 100%|██████████| 4/4 [00:12<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating EM and F1 scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:11<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Results:\n",
      "  Validation Loss: 0.7543\n",
      "  Exact Match: 0.0000\n",
      "  F1 Score: 0.1718\n",
      "New best F1 score! Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 35/35 [08:39<00:00, 14.86s/it, loss=0.7154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average training loss: 0.6947\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 100%|██████████| 4/4 [00:11<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating EM and F1 scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:11<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Results:\n",
      "  Validation Loss: 0.7400\n",
      "  Exact Match: 0.0000\n",
      "  F1 Score: 0.1757\n",
      "New best F1 score! Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 35/35 [10:40<00:00, 18.31s/it, loss=0.6905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average training loss: 0.6411\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 100%|██████████| 4/4 [00:11<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating EM and F1 scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:11<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Results:\n",
      "  Validation Loss: 0.6514\n",
      "  Exact Match: 0.0000\n",
      "  F1 Score: 0.1634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 35/35 [08:38<00:00, 14.83s/it, loss=0.5436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average training loss: 0.6017\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 100%|██████████| 4/4 [00:12<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating EM and F1 scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:12<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Results:\n",
      "  Validation Loss: 0.6632\n",
      "  Exact Match: 0.0000\n",
      "  F1 Score: 0.1346\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Load data\n",
    "print(\"Loading training data...\")\n",
    "with open(\"qa_train.json\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(qa_data)} QA pairs\")\n",
    "\n",
    "# 2. Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item.get(\"question\", \"\")\n",
    "        context = item.get(\"context\", question)\n",
    "        answer = item.get(\"answer\", \"\")\n",
    "\n",
    "        if not question or not context or not answer:\n",
    "            # Return a padded sample for empty data\n",
    "            encoding = self.tokenizer(\n",
    "                \"empty\", \"empty\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_offsets_mapping=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "            encoding[\"start_positions\"] = torch.tensor(0)\n",
    "            encoding[\"end_positions\"] = torch.tensor(0)\n",
    "            encoding.pop(\"offset_mapping\")\n",
    "            return encoding\n",
    "\n",
    "        # Find answer start and end positions in context\n",
    "        start_idx = context.find(answer)\n",
    "        if start_idx == -1:\n",
    "            start_idx = 0  # fallback\n",
    "        end_idx = start_idx + len(answer)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Find token-level start/end positions\n",
    "        offsets = encoding[\"offset_mapping\"][0]\n",
    "        start_position, end_position = 0, 0\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start <= start_idx < end:\n",
    "                start_position = i\n",
    "            if start < end_idx <= end:\n",
    "                end_position = i\n",
    "\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"start_positions\"] = torch.tensor(start_position)\n",
    "        encoding[\"end_positions\"] = torch.tensor(end_position)\n",
    "        encoding.pop(\"offset_mapping\")\n",
    "        return encoding\n",
    "\n",
    "# 3. Evaluation functions\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Calculate F1 score between prediction and ground truth.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Calculate exact match score between prediction and ground truth.\"\"\"\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def evaluate_model(model, tokenizer, data_loader, device, max_length=128):\n",
    "    \"\"\"Evaluate model and return EM and F1 scores.\"\"\"\n",
    "    model.eval()\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(**batch)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            \n",
    "            # Get predicted start and end positions\n",
    "            start_preds = torch.argmax(start_logits, dim=-1)\n",
    "            end_preds = torch.argmax(end_logits, dim=-1)\n",
    "            \n",
    "            # Convert predictions to text\n",
    "            for i in range(len(start_preds)):\n",
    "                start_pos = start_preds[i].item()\n",
    "                end_pos = end_preds[i].item()\n",
    "                \n",
    "                # Get the original context and answer\n",
    "                original_data = data_loader.dataset.data[total_samples + i]\n",
    "                context = original_data.get(\"context\", \"\")\n",
    "                true_answer = original_data.get(\"answer\", \"\")\n",
    "                \n",
    "                # Tokenize context to get offsets\n",
    "                encoding = tokenizer(\n",
    "                    original_data.get(\"question\", \"\"),\n",
    "                    context,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                offsets = encoding[\"offset_mapping\"][0]\n",
    "                \n",
    "                # Extract predicted answer text\n",
    "                if start_pos < len(offsets) and end_pos < len(offsets) and start_pos <= end_pos:\n",
    "                    start_char = offsets[start_pos][0].item()\n",
    "                    end_char = offsets[end_pos][1].item()\n",
    "                    predicted_answer = context[start_char:end_char]\n",
    "                else:\n",
    "                    predicted_answer = \"\"\n",
    "                \n",
    "                # Calculate metrics\n",
    "                em_score = exact_match_score(predicted_answer, true_answer)\n",
    "                f1 = f1_score(predicted_answer, true_answer)\n",
    "                \n",
    "                total_em += em_score\n",
    "                total_f1 += f1\n",
    "            \n",
    "            total_samples += len(start_preds)\n",
    "    \n",
    "    avg_em = total_em / total_samples if total_samples > 0 else 0\n",
    "    avg_f1 = total_f1 / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    return avg_em, avg_f1\n",
    "\n",
    "# 4. Split train/validation set\n",
    "print(\"Splitting data into train/validation sets...\")\n",
    "train_data, val_data = train_test_split(qa_data, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# 5. Load mBERT tokenizer and model\n",
    "print(\"Loading mBERT model and tokenizer...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_dataset = QADataset(train_data, tokenizer)\n",
    "val_dataset = QADataset(val_data, tokenizer)\n",
    "\n",
    "# Use num_workers=2 for better performance\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# 6. Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Add mixed precision training for better performance\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 7. Training loop with EM and F1 tracking\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "best_val_loss = float('inf')\n",
    "best_em = 0\n",
    "best_f1 = 0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} average training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation with EM and F1\n",
    "    print(\"Running validation...\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Loss\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate EM and F1 scores\n",
    "    print(\"Calculating EM and F1 scores...\")\n",
    "    em_score, f1_score_val = evaluate_model(model, tokenizer, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Results:\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Exact Match: {em_score:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_score_val:.4f}\")\n",
    "    \n",
    "    # Save training history\n",
    "    epoch_results = {\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'em_score': em_score,\n",
    "        'f1_score': f1_score_val\n",
    "    }\n",
    "    training_history.append(epoch_results)\n",
    "    \n",
    "    # Save best model based on F1 score (you can change this to EM if preferred)\n",
    "    if f1_score_val > best_f1:\n",
    "        best_f1 = f1_score_val\n",
    "        best_em = em_score\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(f\"New best F1 score! Saving model...\")\n",
    "        model.save_pretrained(\"mbbert-qa-finetuned-best\")\n",
    "        tokenizer.save_pretrained(\"mbbert-qa-finetuned-best\")\n",
    "        \n",
    "        # Save training history\n",
    "        with open(\"training_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "\n",
    "# 8. Save final model\n",
    "model.save_pretrained(\"mbbert-qa-finetuned\")\n",
    "tokenizer.save_pretrained(\"mbbert-qa-finetuned\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
